{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2732ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import zipfile\n",
    "import contextlib\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b71a2e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_FORMATS = 'bmp', 'dng', 'jpeg', 'jpg', 'mpo', 'png', 'tif', 'tiff', 'webp', 'pfm'  # include image suffixes\n",
    "VID_FORMATS = 'asf', 'avi', 'gif', 'm4v', 'mkv', 'mov', 'mp4', 'mpeg', 'mpg', 'ts', 'wmv'  # include video suffixes\n",
    "\n",
    "classes = [\n",
    "'dog',\n",
    "'person',\n",
    "'cat',\n",
    "'tv',\n",
    "'car',\n",
    "'meatballs',\n",
    "'marinara sauce',\n",
    "'tomato soup',\n",
    "'chicken noodle soup',\n",
    "'french onion soup',\n",
    "'chicken breast',\n",
    "'ribs',\n",
    "'pulled pork',\n",
    "'hamburger',\n",
    "'cavity',\n",
    "'rb_33',\n",
    "]    \n",
    "names = {}\n",
    "for i,v in enumerate(classes):\n",
    "    names[i] = v\n",
    "\n",
    "m_path = 'yolov5/best-fp16.tflite'\n",
    "source = 'yolov5/data/images1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15120c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectMultiBackend():\n",
    "    def __init__(self,weights,names):\n",
    "\n",
    "        w = weights\n",
    "        stride = 32  # default stride\n",
    "        Interpreter, load_delegate = tf.lite.Interpreter, tf.lite.experimental.load_delegate,\n",
    "        interpreter = Interpreter(model_path=w)\n",
    "        interpreter.allocate_tensors()  # allocate\n",
    "        input_details = interpreter.get_input_details()  # inputs\n",
    "        output_details = interpreter.get_output_details()  # outputs\n",
    "        with contextlib.suppress(zipfile.BadZipFile):\n",
    "            with zipfile.ZipFile(w, \"r\") as model:\n",
    "                meta_file = model.namelist()[0]\n",
    "                meta = ast.literal_eval(model.read(meta_file).decode(\"utf-8\"))\n",
    "                stride, names = int(meta['stride']), meta['names']\n",
    "        \n",
    "        self.nhwc=True\n",
    "        self.__dict__.update(locals())  # assign all variables to self\n",
    "    \n",
    "    def forward(self, im, augment=False, visualize=False):\n",
    "        # YOLOv5 MultiBackend inference\n",
    "        b, ch, h, w = im.shape  # batch, channel, height, width\n",
    "        if self.nhwc:\n",
    "            im = im.transpose((0, 2, 3, 1))  # torch BCHW to numpy BHWC shape(1,320,192,3)\n",
    "\n",
    "\n",
    "         # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)\n",
    "    \n",
    "        input_ = self.input_details[0]\n",
    "        int8 = input_['dtype'] == np.uint8  # is TFLite quantized uint8 model\n",
    "        if int8:\n",
    "            scale, zero_point = input_['quantization']\n",
    "            im = (im / scale + zero_point).astype(np.uint8)  # de-scale\n",
    "        self.interpreter.set_tensor(input_['index'], im)\n",
    "        self.interpreter.invoke()\n",
    "        y = []\n",
    "        for output in self.output_details:\n",
    "            x = self.interpreter.get_tensor(output['index'])\n",
    "            if int8:\n",
    "                scale, zero_point = output['quantization']\n",
    "                x = (x.astype(np.float32) - zero_point) * scale  # re-scale\n",
    "            y.append(x)\n",
    "        y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]\n",
    "\n",
    "        y[0][..., :4] *= [w, h, w, h]  # xywh normalized to pixels\n",
    "\n",
    "        if isinstance(y, (list, tuple)):\n",
    "            return y[0] if len(y) == 1 else [x for x in y]\n",
    "        else:\n",
    "            return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a40a0a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadImages:\n",
    "    # YOLOv5 image/video dataloader, i.e. `python detect.py --source image.jpg/vid.mp4`\n",
    "    def __init__(self, path, img_size=640, stride=32, auto=True, transforms=None, vid_stride=1):\n",
    "        if isinstance(path, str) and Path(path).suffix == \".txt\":  # *.txt file with img/vid/dir on each line\n",
    "            path = Path(path).read_text().rsplit()\n",
    "        files = []\n",
    "        for p in sorted(path) if isinstance(path, (list, tuple)) else [path]:\n",
    "            p = str(Path(p).resolve())\n",
    "            if '*' in p:\n",
    "                files.extend(sorted(glob.glob(p, recursive=True)))  # glob\n",
    "            elif os.path.isdir(p):\n",
    "                files.extend(sorted(glob.glob(os.path.join(p, '*.*'))))  # dir\n",
    "            elif os.path.isfile(p):\n",
    "                files.append(p)  # files\n",
    "            else:\n",
    "                raise FileNotFoundError(f'{p} does not exist')\n",
    "\n",
    "        images = [x for x in files if x.split('.')[-1].lower() in IMG_FORMATS]\n",
    "        videos = [x for x in files if x.split('.')[-1].lower() in VID_FORMATS]\n",
    "        ni, nv = len(images), len(videos)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.stride = stride\n",
    "        self.files = images + videos\n",
    "        self.nf = ni + nv  # number of files\n",
    "        self.video_flag = [False] * ni + [True] * nv\n",
    "        self.mode = 'image'\n",
    "        self.auto = auto\n",
    "        self.transforms = transforms  # optional\n",
    "        self.vid_stride = vid_stride  # video frame-rate stride\n",
    "        if any(videos):\n",
    "            self._new_video(videos[0])  # new video\n",
    "        else:\n",
    "            self.cap = None\n",
    "        assert self.nf > 0, f'No images or videos found in {p}. ' \\\n",
    "                            f'Supported formats are:\\nimages: {IMG_FORMATS}\\nvideos: {VID_FORMATS}'\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.count = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.count == self.nf:\n",
    "            raise StopIteration\n",
    "        path = self.files[self.count]\n",
    "\n",
    "        if self.video_flag[self.count]:\n",
    "            # Read video\n",
    "            self.mode = 'video'\n",
    "            for _ in range(self.vid_stride):\n",
    "                self.cap.grab()\n",
    "            ret_val, im0 = self.cap.retrieve()\n",
    "            while not ret_val:\n",
    "                self.count += 1\n",
    "                self.cap.release()\n",
    "                if self.count == self.nf:  # last video\n",
    "                    raise StopIteration\n",
    "                path = self.files[self.count]\n",
    "                self._new_video(path)\n",
    "                ret_val, im0 = self.cap.read()\n",
    "\n",
    "            self.frame += 1\n",
    "            # im0 = self._cv2_rotate(im0)  # for use if cv2 autorotation is False\n",
    "            s = f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}: '\n",
    "\n",
    "        else:\n",
    "            # Read image\n",
    "            self.count += 1\n",
    "            im0 = cv2.imread(path)  # BGR\n",
    "            assert im0 is not None, f'Image Not Found {path}'\n",
    "            s = f'image {self.count}/{self.nf} {path}: '\n",
    "\n",
    "        if self.transforms:\n",
    "            im = self.transforms(im0)  # transforms\n",
    "        else:\n",
    "            im = letterbox(im0, self.img_size, stride=self.stride, auto=self.auto)[0]  # padded resize\n",
    "            im = im.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "            im = np.ascontiguousarray(im)  # contiguous\n",
    "\n",
    "        return path, im, im0, self.cap, s\n",
    "\n",
    "    def _new_video(self, path):\n",
    "        # Create a new video capture object\n",
    "        self.frame = 0\n",
    "        self.cap = cv2.VideoCapture(path)\n",
    "        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT) / self.vid_stride)\n",
    "        self.orientation = int(self.cap.get(cv2.CAP_PROP_ORIENTATION_META))  # rotation degrees\n",
    "        # self.cap.set(cv2.CAP_PROP_ORIENTATION_AUTO, 0)  # disable https://github.com/ultralytics/yolov5/issues/8493\n",
    "\n",
    "    def _cv2_rotate(self, im):\n",
    "        # Rotate a cv2 video manually\n",
    "        if self.orientation == 0:\n",
    "            return cv2.rotate(im, cv2.ROTATE_90_CLOCKWISE)\n",
    "        elif self.orientation == 180:\n",
    "            return cv2.rotate(im, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        elif self.orientation == 90:\n",
    "            return cv2.rotate(im, cv2.ROTATE_180)\n",
    "        return im\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nf  # number of files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a1b147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "    elif scaleFill:  # stretch\n",
    "        dw, dh = 0.0, 0.0\n",
    "        new_unpad = (new_shape[1], new_shape[0])\n",
    "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, ratio, (dw, dh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7781f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms_cpu(boxes, scores,  overlap_threshold=0.5, min_mode=False):\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        keep.append(order[0])\n",
    "        xx1 = np.maximum(x1[order[0]], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[order[0]], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[order[0]], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[order[0]], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "        inter = w * h\n",
    "\n",
    "        if min_mode:\n",
    "            ovr = inter / np.minimum(areas[order[0]], areas[order[1:]])\n",
    "        else:\n",
    "            ovr = inter / (areas[order[0]] + areas[order[1:]] - inter)\n",
    "\n",
    "        inds = np.where(ovr <= overlap_threshold)[0]\n",
    "        order = order[inds + 1]\n",
    "    return np.array(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00142d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(\n",
    "        prediction,\n",
    "        conf_thres=0.25,\n",
    "        iou_thres=0.45,\n",
    "        classes=None,\n",
    "        agnostic=False,\n",
    "        multi_label=False,\n",
    "        labels=(),\n",
    "        max_det=300,\n",
    "        nm=0,  # number of masks\n",
    "):\n",
    "    \"\"\"Non-Maximum Suppression (NMS) on inference results to reject overlapping detections\n",
    "\n",
    "    Returns:\n",
    "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(prediction, (list, tuple)):  # YOLOv5 model in validation model, output = (inference_out, loss_out)\n",
    "        prediction = prediction[0]  # select only inference output\n",
    "\n",
    "    bs = prediction.shape[0]  # batch size\n",
    "    nc = prediction.shape[2] - nm - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Checks\n",
    "    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
    "    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
    "\n",
    "    # Settings\n",
    "    # min_wh = 2  # (pixels) minimum box width and height\n",
    "    max_wh = 7680  # (pixels) maximum box width and height\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 0.5 + 0.05 * bs  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "    merge = False  # use merge-NMS\n",
    "\n",
    "    t = time.time()\n",
    "    mi = 5 + nc  # mask start index\n",
    "    output = [np.zeros((0, 6 + nm))] * bs\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]):\n",
    "            lb = labels[xi]\n",
    "            v = np.zeros((len(lb), nc + nm + 5))\n",
    "            v[:, :4] = lb[:, 1:5]  # box\n",
    "            v[:, 4] = 1.0  # conf\n",
    "            v[range(len(lb)), lb[:, 0].long() + 5] = 1.0  # cls\n",
    "            x = np.concatenate((x, v), axis=0)\n",
    "        \n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "        # Box/Mask\n",
    "        box = xywh2xyxy(x[:, :4])  # center_x, center_y, width, height) to (x1, y1, x2, y2)\n",
    "        mask = x[:, mi:]  # zero columns if no masks\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            # bug --seems to be never used\n",
    "            i, j = (x[:, 5:mi] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = np.concatenate((box[i], x[i, 5 + j, None], j[:, None].astype(np.float32), mask[i]), axis=1)\n",
    "        else:  # best class only\n",
    "            probs_x = x[:, 5:mi]\n",
    "            j = probs_x.argmax(axis=1)\n",
    "            conf = np.array([probs_x[i,j[i]] for i in range(probs_x.shape[0])]).reshape(-1,1)\n",
    "            j = j.reshape(-1,1)\n",
    "            #conf, j = x[:, 5:mi].max(1, keepdim=True)\n",
    "            x = np.concatenate((box, conf, j.astype(np.float32), mask), axis=1)[conf.flatten() > conf_thres]\n",
    "        \n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            ind_classes = (x[:, 5:6]==classes).max()\n",
    "            x = x[ind_classes][0]\n",
    "        \n",
    "            #x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "        # Apply finite constraint\n",
    "        # if not torch.isfinite(x).all():\n",
    "        #     x = x[torch.isfinite(x).all(1)]\n",
    "\n",
    "        # Check shape\n",
    "        \n",
    "        \n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        \n",
    "\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            x = x[(-x[:,4]).argsort()[:max_nms]]  # sort by confidence\n",
    "        else:\n",
    "            x = x[(-x[:,4]).argsort()]  # sort by confidence\n",
    "        \n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = nms_cpu(boxes,scores,iou_thres)\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "            weights = iou * scores[None]  # box weights\n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "\n",
    "        if (time.time() - t) > time_limit:\n",
    "            #LOGGER.warning(f'WARNING ⚠️ NMS time limit {time_limit:.3f}s exceeded')\n",
    "            break  # time limit exceeded\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e4297a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Profile(contextlib.ContextDecorator):\n",
    "    # YOLOv5 Profile class. Usage: @Profile() decorator or 'with Profile():' context manager\n",
    "    def __init__(self, t=0.0):\n",
    "        self.t = t\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = self.time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        self.dt = self.time() - self.start  # delta-time\n",
    "        self.t += self.dt  # accumulate dt\n",
    "\n",
    "    def time(self):\n",
    "        return time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59709a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_ascii(s=''):\n",
    "    # Is string composed of all ASCII (no UTF) characters? (note str().isascii() introduced in python 3.7)\n",
    "    s = str(s)  # convert list, tuple, None, etc. to str\n",
    "    return len(s.encode().decode('ascii', 'ignore')) == len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fde8d32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotator:\n",
    "    # YOLOv5 Annotator for train/val mosaics and jpgs and detect/hub inference annotations\n",
    "    def __init__(self, im, line_width=None, font_size=None, font='Arial.ttf', pil=False, example='abc'):\n",
    "        assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to Annotator() input images.'\n",
    "        non_ascii = not is_ascii(example)  # non-latin labels, i.e. asian, arabic, cyrillic\n",
    "        self.pil = pil or non_ascii\n",
    "        if self.pil:  # use PIL\n",
    "            self.im = im if isinstance(im, Image.Image) else Image.fromarray(im)\n",
    "            self.draw = ImageDraw.Draw(self.im)\n",
    "            self.font = check_pil_font(font='Arial.Unicode.ttf' if non_ascii else font,\n",
    "                                       size=font_size or max(round(sum(self.im.size) / 2 * 0.035), 12))\n",
    "        else:  # use cv2\n",
    "            self.im = im\n",
    "        self.lw = line_width or max(round(sum(im.shape) / 2 * 0.003), 2)  # line width\n",
    "\n",
    "    def box_label(self, box, label='', color=(128, 128, 128), txt_color=(255, 255, 255)):\n",
    "        # Add one xyxy box to image with label\n",
    "        if self.pil or not is_ascii(label):\n",
    "            self.draw.rectangle(box, width=self.lw, outline=color)  # box\n",
    "            if label:\n",
    "                w, h = self.font.getsize(label)  # text width, height\n",
    "                outside = box[1] - h >= 0  # label fits outside box\n",
    "                self.draw.rectangle(\n",
    "                    (box[0], box[1] - h if outside else box[1], box[0] + w + 1,\n",
    "                     box[1] + 1 if outside else box[1] + h + 1),\n",
    "                    fill=color,\n",
    "                )\n",
    "                # self.draw.text((box[0], box[1]), label, fill=txt_color, font=self.font, anchor='ls')  # for PIL>8.0\n",
    "                self.draw.text((box[0], box[1] - h if outside else box[1]), label, fill=txt_color, font=self.font)\n",
    "        else:  # cv2\n",
    "            p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
    "            cv2.rectangle(self.im, p1, p2, color, thickness=self.lw, lineType=cv2.LINE_AA)\n",
    "            if label:\n",
    "                tf = max(self.lw - 1, 1)  # font thickness\n",
    "                w, h = cv2.getTextSize(label, 0, fontScale=self.lw / 3, thickness=tf)[0]  # text width, height\n",
    "                outside = p1[1] - h >= 3\n",
    "                p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
    "                cv2.rectangle(self.im, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
    "                cv2.putText(self.im,\n",
    "                            label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2),\n",
    "                            0,\n",
    "                            self.lw / 3,\n",
    "                            txt_color,\n",
    "                            thickness=tf,\n",
    "                            lineType=cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    def rectangle(self, xy, fill=None, outline=None, width=1):\n",
    "        # Add rectangle to image (PIL-only)\n",
    "        self.draw.rectangle(xy, fill, outline, width)\n",
    "\n",
    "    def text(self, xy, text, txt_color=(255, 255, 255), anchor='top'):\n",
    "        # Add text to image (PIL-only)\n",
    "        if anchor == 'bottom':  # start y from font bottom\n",
    "            w, h = self.font.getsize(text)  # text width, height\n",
    "            xy[1] += 1 - h\n",
    "        self.draw.text(xy, text, fill=txt_color, font=self.font)\n",
    "\n",
    "    def fromarray(self, im):\n",
    "        # Update self.im from a numpy array\n",
    "        self.im = im if isinstance(im, Image.Image) else Image.fromarray(im)\n",
    "        self.draw = ImageDraw.Draw(self.im)\n",
    "\n",
    "    def result(self):\n",
    "        # Return annotated image as array\n",
    "        return np.asarray(self.im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0c365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbbe035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_path = 'yolov5/best-fp16.tflite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d39d98d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DetectMultiBackend(m_path,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9625c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride, names, pt = model.stride, model.names, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bddf7df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4a9e135",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LoadImages(source,auto=pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe0ef211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.LoadImages at 0x22f2f14cdc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a86d6120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy2xywh(x):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n",
    "    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n",
    "    y[:, 2] = x[:, 2] - x[:, 0]  # width\n",
    "    y[:, 3] = x[:, 3] - x[:, 1]  # height\n",
    "    return y\n",
    "\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y =  np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def xywhn2xyxy(x, w=640, h=640, padw=0, padh=0):\n",
    "    # Convert nx4 boxes from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + padw  # top left x\n",
    "    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + padh  # top left y\n",
    "    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + padw  # bottom right x\n",
    "    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + padh  # bottom right y\n",
    "    return y\n",
    "\n",
    "\n",
    "def xyxy2xywhn(x, w=640, h=640, clip=False, eps=0.0):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
    "    if clip:\n",
    "        clip_boxes(x, (h - eps, w - eps))  # warning: inplace clip\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n",
    "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n",
    "    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n",
    "    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n",
    "    return y\n",
    "\n",
    "\n",
    "def xyn2xy(x, w=640, h=640, padw=0, padh=0):\n",
    "    # Convert normalized segments into pixel segments, shape (n,2)\n",
    "    y =  np.copy(x)\n",
    "    y[:, 0] = w * x[:, 0] + padw  # top left x\n",
    "    y[:, 1] = h * x[:, 1] + padh  # top left y\n",
    "    return y\n",
    "\n",
    "def scale_boxes(img1_shape, boxes, img0_shape, ratio_pad=None):\n",
    "    # Rescale boxes (xyxy) from img1_shape to img0_shape\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    boxes[:, [0, 2]] -= pad[0]  # x padding\n",
    "    boxes[:, [1, 3]] -= pad[1]  # y padding\n",
    "    boxes[:, :4] /= gain\n",
    "    clip_boxes(boxes, img0_shape)\n",
    "    return boxes\n",
    "\n",
    "def clip_boxes(boxes, shape):\n",
    "    # Clip boxes (xyxy) to image shape (height, width)\n",
    "    boxes[:, [0, 2]] = boxes[:, [0, 2]].clip(0, shape[1])  # x1, x2\n",
    "    boxes[:, [1, 3]] = boxes[:, [1, 3]].clip(0, shape[0])  # y1, y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40c96b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colors:\n",
    "    # Ultralytics color palette https://ultralytics.com/\n",
    "    def __init__(self):\n",
    "        # hex = matplotlib.colors.TABLEAU_COLORS.values()\n",
    "        hexs = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n",
    "                '2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\n",
    "        self.palette = [self.hex2rgb(f'#{c}') for c in hexs]\n",
    "        self.n = len(self.palette)\n",
    "\n",
    "    def __call__(self, i, bgr=False):\n",
    "        c = self.palette[int(i) % self.n]\n",
    "        return (c[2], c[1], c[0]) if bgr else c\n",
    "\n",
    "    @staticmethod\n",
    "    def hex2rgb(h):  # rgb order (PIL)\n",
    "        return tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7dd010f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = Colors()  # create instance for 'from utils.plots import colors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d0879c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_on_dataset():\n",
    "    view_img=False  # show results\n",
    "    save_crop=False\n",
    "    save_img=True   \n",
    "    save_conf=True #save confidences in --save-txt labels\n",
    "\n",
    "    save_txt=True\n",
    "    hide_labels = False\n",
    "    hide_conf=False\n",
    "    line_thickness=3\n",
    "    seen, windows, dt = 0, [], (Profile(), Profile(), Profile())\n",
    "    for path, im, im0s, vid_cap, s in dataset:\n",
    "        with dt[0]:\n",
    "            im = im.astype(np.float32)  # uint8 to fp16/32\n",
    "            im /= 255  # 0 - 255 to 0.0 - 1.0\n",
    "            if len(im.shape) == 3:\n",
    "                im = im[None]  # expand for batch dim\n",
    "\n",
    "        # Inference\n",
    "        with dt[1]:\n",
    "            visualize = False\n",
    "            augment=False\n",
    "            pred = model.forward(im, augment=augment, visualize=visualize)\n",
    "\n",
    "\n",
    "        # NMS\n",
    "        with dt[2]:\n",
    "            classes = np.array(list(names.keys()))\n",
    "            pred = non_max_suppression(pred, classes=classes)\n",
    "        # Second-stage classifier (optional)\n",
    "        # pred = utils.general.apply_classifier(pred, classifier_model, im, im0s)\n",
    "\n",
    "        # Process predictions\n",
    "        for i, det in enumerate(pred):  # per image\n",
    "            seen += 1\n",
    "            p, im0, frame = path, im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "\n",
    "            p = Path(p)  # to Path\n",
    "            save_dir = Path('tmp')\n",
    "            save_path = str(save_dir / p.name)  # im.jpg\n",
    "            txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # im.txt\n",
    "            s += '%gx%g ' % im.shape[2:]  # print string\n",
    "            gn = np.array([im0.shape[1],im0.shape[0],im0.shape[1],im0.shape[0]])  # normalization gain whwh\n",
    "            imc = im0.copy() if save_crop else im0  # for save_crop\n",
    "            annotator = Annotator(im0, line_width=line_thickness, example=str(names))\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_boxes(im.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    " \n",
    "                for c in np.unique(det[:, 5]):\n",
    "                    n = (det[:, 5] == c).sum()  # detections per class\n",
    "                    s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "                # Write results\n",
    "                for *xyxy, conf, cls in reversed(det):\n",
    "                    if save_txt:  # Write to file\n",
    "                        xywh = (xyxy2xywh(np.array(xyxy).reshape(1, 4)) / gn).flatten().tolist()  # normalized xywh\n",
    "                        line = (cls, *xywh, conf) if save_conf else (cls, *xywh)  # label format\n",
    "                        with open(f'{txt_path}.txt', 'a') as f:\n",
    "                            f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                    if save_img or save_crop or view_img:  # Add bbox to image\n",
    "                        c = int(cls)  # integer class\n",
    "                        label = None if hide_labels else (names[c] if hide_conf else f'{names[c]} {conf:.2f}')\n",
    "                        annotator.box_label(xyxy, label, color=colors(c, True))\n",
    "                    if save_crop:\n",
    "                        save_one_box(xyxy, imc, file=save_dir / 'crops' / names[c] / f'{p.stem}.jpg', BGR=True)\n",
    "\n",
    "            # Stream results\n",
    "            im0 = annotator.result()\n",
    "            if view_img:\n",
    "                if platform.system() == 'Linux' and p not in windows:\n",
    "                    windows.append(p)\n",
    "                    cv2.namedWindow(str(p), cv2.WINDOW_NORMAL | cv2.WINDOW_KEEPRATIO)  # allow window resize (Linux)\n",
    "                    cv2.resizeWindow(str(p), im0.shape[1], im0.shape[0])\n",
    "                cv2.imshow(str(p), im0)\n",
    "                cv2.waitKey(1)  # 1 millisecond\n",
    "\n",
    "            # Save results (image with detections)\n",
    "            if save_img:\n",
    "                if dataset.mode == 'image':\n",
    "                    cv2.imwrite(save_path, im0)\n",
    "                else:  # 'video' or 'stream'\n",
    "                    if vid_path[i] != save_path:  # new video\n",
    "                        vid_path[i] = save_path\n",
    "                        if isinstance(vid_writer[i], cv2.VideoWriter):\n",
    "                            vid_writer[i].release()  # release previous video writer\n",
    "                        if vid_cap:  # video\n",
    "                            fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                            w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                            h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                        else:  # stream\n",
    "                            fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                        save_path = str(Path(save_path).with_suffix('.mp4'))  # force *.mp4 suffix on results videos\n",
    "                        vid_writer[i] = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                    vid_writer[i].write(im0)\n",
    "\n",
    "        # Print time (inference-only)\n",
    "        #LOGGER.info(f\"{s}{'' if len(det) else '(no detections), '}{dt[1].dt * 1E3:.1f}ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c60675e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Timofey.Voytko\\AppData\\Local\\Temp\\ipykernel_15352\\1611109454.py:54: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n"
     ]
    }
   ],
   "source": [
    "inference_on_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b1bdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
